{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.5/site-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# import torchvision\n",
    "# from torchvision import models\n",
    "from torchvision.ops import misc as misc_nn_ops\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "\n",
    "from torchvision.models.utils import load_state_dict_from_url\n",
    "\n",
    "from generalized_rcnn import GeneralizedRCNN\n",
    "from rpn import AnchorGenerator, RPNHead, RegionProposalNetwork\n",
    "from roi_heads import RoIHeads\n",
    "from transform import GeneralizedRCNNTransform\n",
    "from backbone_utils import resnet_fpn_backbone\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\n",
    "    \"FasterRCNN\", \"fasterrcnn_resnet50_fpn\",\n",
    "]\n",
    "\n",
    "\n",
    "class FasterRCNN(GeneralizedRCNN):\n",
    "    \"\"\"\n",
    "    Implements Faster R-CNN.\n",
    "\n",
    "    The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each\n",
    "    image, and should be in 0-1 range. Different images can have different sizes.\n",
    "\n",
    "    The behavior of the model changes depending if it is in training or evaluation mode.\n",
    "\n",
    "    During training, the model expects both the input tensors, as well as a targets (list of dictionary),\n",
    "    containing:\n",
    "        - boxes (FloatTensor[N, 4]): the ground-truth boxes in [x1, y1, x2, y2] format, with values\n",
    "          between 0 and H and 0 and W\n",
    "        - labels (Int64Tensor[N]): the class label for each ground-truth box\n",
    "\n",
    "    The model returns a Dict[Tensor] during training, containing the classification and regression\n",
    "    losses for both the RPN and the R-CNN.\n",
    "\n",
    "    During inference, the model requires only the input tensors, and returns the post-processed\n",
    "    predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as\n",
    "    follows:\n",
    "        - boxes (FloatTensor[N, 4]): the predicted boxes in [x1, y1, x2, y2] format, with values between\n",
    "          0 and H and 0 and W\n",
    "        - labels (Int64Tensor[N]): the predicted labels for each image\n",
    "        - scores (Tensor[N]): the scores or each prediction\n",
    "\n",
    "    Arguments:\n",
    "        backbone (nn.Module): the network used to compute the features for the model.\n",
    "            It should contain a out_channels attribute, which indicates the number of output\n",
    "            channels that each feature map has (and it should be the same for all feature maps).\n",
    "            The backbone should return a single Tensor or and OrderedDict[Tensor].\n",
    "        num_classes (int): number of output classes of the model (including the background).\n",
    "            If box_predictor is specified, num_classes should be None.\n",
    "        min_size (int): minimum size of the image to be rescaled before feeding it to the backbone\n",
    "        max_size (int): maximum size of the image to be rescaled before feeding it to the backbone\n",
    "        image_mean (Tuple[float, float, float]): mean values used for input normalization.\n",
    "            They are generally the mean values of the dataset on which the backbone has been trained\n",
    "            on\n",
    "        image_std (Tuple[float, float, float]): std values used for input normalization.\n",
    "            They are generally the std values of the dataset on which the backbone has been trained on\n",
    "        rpn_anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature\n",
    "            maps.\n",
    "        rpn_head (nn.Module): module that computes the objectness and regression deltas from the RPN\n",
    "        rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training\n",
    "        rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing\n",
    "        rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training\n",
    "        rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing\n",
    "        rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals\n",
    "        rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be\n",
    "            considered as positive during training of the RPN.\n",
    "        rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be\n",
    "            considered as negative during training of the RPN.\n",
    "        rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN\n",
    "            for computing the loss\n",
    "        rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training\n",
    "            of the RPN\n",
    "        box_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in\n",
    "            the locations indicated by the bounding boxes\n",
    "        box_head (nn.Module): module that takes the cropped feature maps as input\n",
    "        box_predictor (nn.Module): module that takes the output of box_head and returns the\n",
    "            classification logits and box regression deltas.\n",
    "        box_score_thresh (float): during inference, only return proposals with a classification score\n",
    "            greater than box_score_thresh\n",
    "        box_nms_thresh (float): NMS threshold for the prediction head. Used during inference\n",
    "        box_detections_per_img (int): maximum number of detections per image, for all classes.\n",
    "        box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be\n",
    "            considered as positive during training of the classification head\n",
    "        box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be\n",
    "            considered as negative during training of the classification head\n",
    "        box_batch_size_per_image (int): number of proposals that are sampled during training of the\n",
    "            classification head\n",
    "        box_positive_fraction (float): proportion of positive proposals in a mini-batch during training\n",
    "            of the classification head\n",
    "        bbox_reg_weights (Tuple[float, float, float, float]): weights for the encoding/decoding of the\n",
    "            bounding boxes\n",
    "\n",
    "    Example::\n",
    "\n",
    "        >>> import torch\n",
    "        >>> import torchvision\n",
    "        >>> from torchvision.models.detection import FasterRCNN\n",
    "        >>> from torchvision.models.detection.rpn import AnchorGenerator\n",
    "        >>> # load a pre-trained model for classification and return\n",
    "        >>> # only the features\n",
    "        >>> backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "        >>> # FasterRCNN needs to know the number of\n",
    "        >>> # output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "        >>> # so we need to add it here\n",
    "        >>> backbone.out_channels = 1280\n",
    "        >>>\n",
    "        >>> # let's make the RPN generate 5 x 3 anchors per spatial\n",
    "        >>> # location, with 5 different sizes and 3 different aspect\n",
    "        >>> # ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "        >>> # map could potentially have different sizes and\n",
    "        >>> # aspect ratios\n",
    "        >>> anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "        >>>                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "        >>>\n",
    "        >>> # let's define what are the feature maps that we will\n",
    "        >>> # use to perform the region of interest cropping, as well as\n",
    "        >>> # the size of the crop after rescaling.\n",
    "        >>> # if your backbone returns a Tensor, featmap_names is expected to\n",
    "        >>> # be [0]. More generally, the backbone should return an\n",
    "        >>> # OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "        >>> # feature maps to use.\n",
    "        >>> roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "        >>>                                                 output_size=7,\n",
    "        >>>                                                 sampling_ratio=2)\n",
    "        >>>\n",
    "        >>> # put the pieces together inside a FasterRCNN model\n",
    "        >>> model = FasterRCNN(backbone,\n",
    "        >>>                    num_classes=2,\n",
    "        >>>                    rpn_anchor_generator=anchor_generator,\n",
    "        >>>                    box_roi_pool=roi_pooler)\n",
    "        >>> model.eval()\n",
    "        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "        >>> predictions = model(x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone, num_classes=None,\n",
    "                 # transform parameters\n",
    "                 min_size=800, max_size=1333,\n",
    "                 image_mean=None, image_std=None,\n",
    "                 # RPN parameters\n",
    "                 rpn_anchor_generator=None, rpn_head=None,\n",
    "                 rpn_pre_nms_top_n_train=2000, rpn_pre_nms_top_n_test=1000,\n",
    "                 rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=1000,\n",
    "                 rpn_nms_thresh=0.7,\n",
    "                 rpn_fg_iou_thresh=0.7, rpn_bg_iou_thresh=0.3,\n",
    "                 rpn_batch_size_per_image=256, rpn_positive_fraction=0.5,\n",
    "                 # Box parameters\n",
    "                 box_roi_pool=None, box_head=None, box_predictor=None,\n",
    "                 box_score_thresh=0.05, box_nms_thresh=0.5, box_detections_per_img=100,\n",
    "                 box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5,\n",
    "                 box_batch_size_per_image=512, box_positive_fraction=0.25,\n",
    "                 bbox_reg_weights=None):\n",
    "        print(\"RCNN\")\n",
    "        if not hasattr(backbone, \"out_channels\"):\n",
    "            raise ValueError(\n",
    "                \"backbone should contain an attribute out_channels \"\n",
    "                \"specifying the number of output channels (assumed to be the \"\n",
    "                \"same for all the levels)\")\n",
    "\n",
    "        assert isinstance(rpn_anchor_generator, (AnchorGenerator, type(None)))\n",
    "        assert isinstance(box_roi_pool, (MultiScaleRoIAlign, type(None)))\n",
    "\n",
    "        if num_classes is not None:\n",
    "            if box_predictor is not None:\n",
    "                raise ValueError(\"num_classes should be None when box_predictor is specified\")\n",
    "        else:\n",
    "            if box_predictor is None:\n",
    "                raise ValueError(\"num_classes should not be None when box_predictor \"\n",
    "                                 \"is not specified\")\n",
    "\n",
    "        out_channels = backbone.out_channels\n",
    "\n",
    "        if rpn_anchor_generator is None:\n",
    "            anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n",
    "            aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "            rpn_anchor_generator = AnchorGenerator(\n",
    "                anchor_sizes, aspect_ratios\n",
    "            )\n",
    "        if rpn_head is None:\n",
    "            rpn_head = RPNHead(\n",
    "                out_channels, rpn_anchor_generator.num_anchors_per_location()[0]\n",
    "            )\n",
    "\n",
    "        rpn_pre_nms_top_n = dict(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)\n",
    "        rpn_post_nms_top_n = dict(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)\n",
    "\n",
    "        rpn = RegionProposalNetwork(\n",
    "            rpn_anchor_generator, rpn_head,\n",
    "            rpn_fg_iou_thresh, rpn_bg_iou_thresh,\n",
    "            rpn_batch_size_per_image, rpn_positive_fraction,\n",
    "            rpn_pre_nms_top_n, rpn_post_nms_top_n, rpn_nms_thresh)\n",
    "\n",
    "        if box_roi_pool is None:\n",
    "            box_roi_pool = MultiScaleRoIAlign(\n",
    "                featmap_names=['0', '1', '2', '3'],\n",
    "                output_size=7,\n",
    "                sampling_ratio=2)\n",
    "\n",
    "        if box_head is None:\n",
    "            resolution = box_roi_pool.output_size[0]\n",
    "            representation_size = 1024\n",
    "            box_head = TwoMLPHead(\n",
    "                out_channels * resolution ** 2,\n",
    "                representation_size)\n",
    "\n",
    "        if box_predictor is None:\n",
    "            representation_size = 1024\n",
    "            box_predictor = FastRCNNPredictor(\n",
    "                representation_size,\n",
    "                num_classes)\n",
    "\n",
    "        roi_heads = RoIHeads(\n",
    "            # Box\n",
    "            box_roi_pool, box_head, box_predictor,\n",
    "            box_fg_iou_thresh, box_bg_iou_thresh,\n",
    "            box_batch_size_per_image, box_positive_fraction,\n",
    "            bbox_reg_weights,\n",
    "            box_score_thresh, box_nms_thresh, box_detections_per_img)\n",
    "\n",
    "        if image_mean is None:\n",
    "            image_mean = [0.485, 0.456, 0.406]\n",
    "        if image_std is None:\n",
    "            image_std = [0.229, 0.224, 0.225]\n",
    "        transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std)\n",
    "\n",
    "        super(FasterRCNN, self).__init__(backbone, rpn, roi_heads, transform)\n",
    "\n",
    "\n",
    "class TwoMLPHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard heads for FPN-based models\n",
    "\n",
    "    Arguments:\n",
    "        in_channels (int): number of input channels\n",
    "        representation_size (int): size of the intermediate representation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, representation_size):\n",
    "        super(TwoMLPHead, self).__init__()\n",
    "\n",
    "        self.fc6 = nn.Linear(in_channels, representation_size)\n",
    "        self.fc7 = nn.Linear(representation_size, representation_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FastRCNNPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard classification + bounding box regression layers\n",
    "    for Fast R-CNN.\n",
    "\n",
    "    Arguments:\n",
    "        in_channels (int): number of input channels\n",
    "        num_classes (int): number of output classes (including background)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(FastRCNNPredictor, self).__init__()\n",
    "        self.cls_score = nn.Linear(in_channels, num_classes)\n",
    "        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            assert list(x.shape[2:]) == [1, 1]\n",
    "        x = x.flatten(start_dim=1)\n",
    "        scores = self.cls_score(x)\n",
    "        bbox_deltas = self.bbox_pred(x)\n",
    "\n",
    "        return scores, bbox_deltas\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'fasterrcnn_resnet50_fpn_coco':\n",
    "        'https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def fasterrcnn_resnet50_fpn(pretrained=False, progress=True,\n",
    "                            num_classes=91, pretrained_backbone=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a Faster R-CNN model with a ResNet-50-FPN backbone.\n",
    "\n",
    "    The input to the model is expected to be a list of tensors, each of shape ``[C, H, W]``, one for each\n",
    "    image, and should be in ``0-1`` range. Different images can have different sizes.\n",
    "\n",
    "    The behavior of the model changes depending if it is in training or evaluation mode.\n",
    "\n",
    "    During training, the model expects both the input tensors, as well as a targets (list of dictionary),\n",
    "    containing:\n",
    "        - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with values\n",
    "          between ``0`` and ``H`` and ``0`` and ``W``\n",
    "        - labels (``Int64Tensor[N]``): the class label for each ground-truth box\n",
    "\n",
    "    The model returns a ``Dict[Tensor]`` during training, containing the classification and regression\n",
    "    losses for both the RPN and the R-CNN.\n",
    " \n",
    "    During inference, the model requires only the input tensors, and returns the post-processed\n",
    "    predictions as a ``List[Dict[Tensor]]``, one for each input image. The fields of the ``Dict`` are as\n",
    "    follows:\n",
    "        - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with values between\n",
    "          ``0`` and ``H`` and ``0`` and ``W``\n",
    "        - labels (``Int64Tensor[N]``): the predicted labels for each image\n",
    "        - scores (``Tensor[N]``): the scores or each prediction\n",
    "\n",
    "    Example::\n",
    "\n",
    "        >>> model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "        >>> model.eval()\n",
    "        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "        >>> predictions = model(x)\n",
    "\n",
    "    Arguments:\n",
    "        pretrained (bool): If True, returns a model pre-trained on COCO train2017\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        # no need to download the backbone if pretrained is set\n",
    "        pretrained_backbone = False\n",
    "    backbone = resnet_fpn_backbone('resnet50', pretrained_backbone)\n",
    "    model = FasterRCNN(backbone, num_classes, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls['fasterrcnn_resnet50_fpn_coco'],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RCNN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform()\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign()\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torchvision\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "# x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "# predictions = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"images/overpass.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "videopath = 'images/overpass.mp4'\n",
    "Video(videopath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "videopath = 'images/overpass.mp4'\n",
    "import sys\n",
    "sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
    "\n",
    "%pylab inline \n",
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# cmap = plt.get_cmap('tab20b')\n",
    "# colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]\n",
    "# from sort import *\n",
    "# img_size=416\n",
    "# conf_thres=0.8\n",
    "# nms_thres=0.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "812 1280 720 30.0\n"
     ]
    }
   ],
   "source": [
    "vid = cv2.VideoCapture(videopath)\n",
    "length = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "width  = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps    = vid.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "print(length, width,height,fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def get_prediction(img_path, threshold,model):\n",
    "#   img = Image.open(img_path) # Load the image\n",
    "  img = img_path\n",
    "  transform = transforms.Compose([transforms.ToTensor()]) # Defing PyTorch Transform\n",
    "  img = transform(img) # Apply the transform to the image\n",
    "  pred = model([img]) # Pass the image to the model\n",
    "#   print(pred)\n",
    "  pred_score = list(pred['scores'].detach().numpy())\n",
    "  pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred['labels'].numpy())] # Get the Prediction Score\n",
    "  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred['boxes'].detach().numpy())] # Bounding boxes\n",
    "  pred_ids = list(pred['tracker_id'].detach().numpy())\n",
    "  pred_t = [pred_score.index(x) for x in pred_score if x > threshold][-1] # Get list of index with score greater than threshold.\n",
    "  pred_boxes = pred_boxes[:pred_t+1]\n",
    "  pred_class = pred_class[:pred_t+1]\n",
    "  return pred_boxes, pred_class#,pred_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection_api(img_path, model, threshold=0.9, rect_th=3, text_size=3, text_th=3):\n",
    " \n",
    "#   boxes, pred_cls, pred_ids = get_prediction(img_path, threshold, model) # Get predictions\n",
    "  boxes, pred_cls = get_prediction(img_path, threshold, model) # Get predictions\n",
    "  img = numpy.array(img_path) \n",
    "#   img = cv2.imread(img_path) # Read image with cv2\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert to RGB\n",
    "  for i in range(len(boxes)):\n",
    "    cv2.rectangle(img, boxes[i][0], boxes[i][1],color=(0, 255, 0), thickness=rect_th) # Draw Rectangle with the coordinates\n",
    "    cv2.putText(img,str(pred_cls[i]), boxes[i][0],  cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th) # Write the prediction class\n",
    "  plt.figure(figsize=(20,30)) # display the output image\n",
    "  plt.imshow(img)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KALMAN-FILTER FAST-RCNN replacing the RPN and ROI for 5 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run features through the backbone\n",
      "Run features through the proposals:\n",
      "Run features through the roi:\n",
      "format for detector_losses {}\n",
      "trans_detection tensor([[5.2838e+02, 3.7323e+02, 5.7691e+02, 4.2361e+02, 9.8293e-01, 9.8293e-01,\n",
      "         3.0000e+00],\n",
      "        [5.0355e+02, 3.3597e+02, 5.3647e+02, 3.6129e+02, 9.7120e-01, 9.7120e-01,\n",
      "         3.0000e+00],\n",
      "        [5.7040e+02, 2.9460e+02, 5.9335e+02, 3.1044e+02, 9.1801e-01, 9.1801e-01,\n",
      "         3.0000e+00],\n",
      "        [6.1732e+02, 2.6373e+02, 6.2939e+02, 2.7673e+02, 8.8210e-01, 8.8210e-01,\n",
      "         3.0000e+00],\n",
      "        [7.5487e+02, 2.7003e+02, 7.6602e+02, 2.8954e+02, 8.2724e-01, 8.2724e-01,\n",
      "         1.0000e+00],\n",
      "        [1.1371e+03, 2.9848e+02, 1.1953e+03, 3.1989e+02, 7.0216e-01, 7.0216e-01,\n",
      "         1.5000e+01]])\n",
      "tracked [[1.13713867e+03 2.98477920e+02 1.19532227e+03 3.19890488e+02\n",
      "  6.00000000e+00 1.00000000e+00]\n",
      " [7.54873535e+02 2.70028213e+02 7.66017822e+02 2.89539291e+02\n",
      "  5.00000000e+00 1.00000000e+00]\n",
      " [6.17321045e+02 2.63728058e+02 6.29388916e+02 2.76732574e+02\n",
      "  4.00000000e+00 1.00000000e+00]\n",
      " [5.70398895e+02 2.94603043e+02 5.93348419e+02 3.10438461e+02\n",
      "  3.00000000e+00 1.00000000e+00]\n",
      " [5.03554703e+02 3.35972672e+02 5.36474594e+02 3.61290268e+02\n",
      "  2.00000000e+00 1.00000000e+00]\n",
      " [5.28379395e+02 3.73230057e+02 5.76909302e+02 4.23608444e+02\n",
      "  1.00000000e+00 1.00000000e+00]]\n",
      "1\n",
      "{'labels': tensor([1, 1, 1, 1, 1, 1], dtype=torch.int32), 'boxes': tensor([[1137.1387,  298.4779, 1195.3223,  319.8905],\n",
      "        [ 754.8735,  270.0282,  766.0178,  289.5393],\n",
      "        [ 617.3210,  263.7281,  629.3889,  276.7326],\n",
      "        [ 570.3989,  294.6031,  593.3484,  310.4384],\n",
      "        [ 503.5547,  335.9727,  536.4746,  361.2903],\n",
      "        [ 528.3794,  373.2301,  576.9093,  423.6084]]), 'tracker_id': tensor([6, 5, 4, 3, 2, 1], dtype=torch.int32)}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-581ef802cd6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpilimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mobject_detection_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpilimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrect_th\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_th\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-5fbd379889c4>\u001b[0m in \u001b[0;36mobject_detection_api\u001b[0;34m(img_path, model, threshold, rect_th, text_size, text_th)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#   boxes, pred_cls, pred_ids = get_prediction(img_path, threshold, model) # Get predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#   img = cv2.imread(img_path) # Read image with cv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-99371950844e>\u001b[0m in \u001b[0;36mget_prediction\u001b[0;34m(img_path, threshold, model)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Pass the image to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#   print(pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mpred_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mpred_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCOCO_INSTANCE_CATEGORY_NAMES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Get the Prediction Score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mpred_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'boxes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Bounding boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "vid = cv2.VideoCapture(videopath)\n",
    "\n",
    "for ii in range(20):\n",
    "    ret, frame = vid.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pilimg = Image.fromarray(frame)\n",
    "    object_detection_api(pilimg,model, rect_th=2, text_th=1, text_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import transforms\n",
    "# dir(transforms)\n",
    "\n",
    "# # torchvision.__version__\n",
    "i=[0,1,2,3,4]\n",
    "\n",
    "i[0:4]+[9,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import transforms\n",
    "# import inspect\n",
    "# print(transforms.trasforms.batch_images(images, size_divisible=32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture(videopath)\n",
    "length = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "width  = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps    = vid.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "print(length, width,height,fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot_tracker = Sort() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# initialize Sort object and video capture\n",
    "\n",
    "#while(True):\n",
    "for ii in range(40):\n",
    "    print(ii)\n",
    "    ret, frame = vid.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pilimg = Image.fromarray(frame)\n",
    "    if ii%5 == 0:\n",
    "        print(\"prediction made\")\n",
    "        detections = detect_image(pilimg)\n",
    "        print(detections)\n",
    "    img = np.array(pilimg)\n",
    "    pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
    "    pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
    "    unpad_h = img_size - pad_y\n",
    "    unpad_w = img_size - pad_x\n",
    "    if detections is not None:\n",
    "        tracked_objects = mot_tracker.update(detections.cpu())\n",
    "        print(\"tracked object:\",tracked_objects, \"detection:\", detections)\n",
    "        \n",
    "                    \n",
    "\n",
    "        unique_labels = detections[:, -1].cpu().unique()\n",
    "        n_cls_preds = len(unique_labels)\n",
    "        for x1, y1, x2, y2, obj_id, cls_pred in tracked_objects:\n",
    "            box_h = int(((y2 - y1) / unpad_h) * img.shape[0])\n",
    "            box_w = int(((x2 - x1) / unpad_w) * img.shape[1])\n",
    "            y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])\n",
    "            x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])\n",
    "\n",
    "            color = colors[int(obj_id) % len(colors)]\n",
    "            color = [i * 255 for i in color]\n",
    "            cls = classes[int(cls_pred)]\n",
    "            cv2.rectangle(frame, (x1, y1), (x1+box_w, y1+box_h), color, 4)\n",
    "            cv2.rectangle(frame, (x1, y1-35), (x1+len(cls)*19+60, y1), color, -1)\n",
    "            cv2.putText(frame, cls + \"-\" + str(int(obj_id)), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 3)\n",
    "        for i in detections:\n",
    "            print(i)\n",
    "            for j in tracked_objects:\n",
    "                if abs(i[0]-j[0]) < 1.5 and abs(i[1]-j[1]) < 1.5 and abs(i[2]-j[2]) < 1.5 and abs(i[3]-j[3]) < 1.5:\n",
    "                    i[0],i[1],i[2],i[3]=j[0],j[1],j[2],j[3]\n",
    "        print(\"new detection\",detections) \n",
    "            \n",
    "    fig=figure(figsize=(12, 8))\n",
    "    title(\"Video Stream\")\n",
    "    imshow(frame)\n",
    "    show()\n",
    "      \n",
    "    time.sleep(2)\n",
    "#     clear_output(wait=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_size = (800//16)\n",
    "ctr_x = np.arange(16, (fe_size+1) * 16, 16)\n",
    "ctr_y = np.arange(16, (fe_size+1) * 16, 16)\n",
    "print(fe_size, \n",
    "      from PIL import Image\n",
    "\n",
    "def get_prediction(img_path, threshold,model):\n",
    "#   img = Image.open(img_path) # Load the image\n",
    "  img = img_path\n",
    "  transform = transforms.Compose([transforms.ToTensor()]) # Defing PyTorch Transform\n",
    "  img = transform(img) # Apply the transform to the image\n",
    "  pred = model([img]) # Pass the image to the model\n",
    "#   print(\"from here\",pred)\n",
    "  pred_class = [COCO_INSTANCE_CATEGORY_NAMES[int(i)] for i in pred['labels']] # Get the Prediction Score\n",
    "  prin\n",
    "  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in pred['boxes']] # Bounding boxes\n",
    "  pred_score = pred['scores']\n",
    "  pred_t = [pred_score.index(x) for x in pred_score if x > threshold][-1] # Get list of index with score greater than threshold.\n",
    "  pred_boxes = pred_boxes[:pred_t+1]\n",
    "  pred_class = pred_class[:pred_t+1]\n",
    "  return pred_boxes, pred_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
